#!/usr/bin/env python3
"""
Comprehensive Test Script for Prompt Gen MCP Server

This script tests that all 47+ prompt engineering techniques are properly loaded
and that the technique selection system is working correctly.
"""

import asyncio
import os
import sys
import json
from pathlib import Path

# Add the src directory to Python path
sys.path.insert(0, str(Path(__file__).parent / "src"))

from prompt_gen_mcp.server import (
    load_llms_txt_techniques,
    select_optimal_techniques,
    llms_techniques,
    get_comprehensive_fallback_techniques,
    initialize_components
)

async def test_technique_loading():
    """Test that all techniques are properly loaded"""
    print("üß™ Testing technique loading...")
    
    # Test loading techniques
    await load_llms_txt_techniques()
    
    print(f"‚úÖ Loaded {len(llms_techniques)} techniques")
    
    # Verify we have all expected techniques (using actual names from llms.txt)
    expected_techniques = [
        "Emotional Language",
        "Role Assignment (Role Prompting)", 
        "Style Definition",
        "Prompt Refinement (Automatic Prompt Optimization)",
        "Perspective Simulation (SimToM)",
        "Ambiguity Clarification (Recourse and Refinement - RAR)",
        "Query Repetition (Re2)",
        "Follow-Up Generation (Self-Ask)",
        "Example Generation (SG-ICL)",
        "Example Ordering",
        "K-Nearest Neighbors (KNN) Example Selection",
        "Vote-K Selection",
        "Analogical Chain of Thought (Analogical CoT)",
        "Step-Back Prompting",
        "Thread of Thought (ToT)",
        "Tabular Chain of Thought (Tabular CoT)",
        "Active Prompting",
        "Auto-CoT",
        "Complexity-Based Chain of Thought",
        "Contrastive Chain of Thought",
        "Memory of Thought (MoT)",
        "Uncertainty-Routed Chain of Thought",
        "Prompt Mining",
        "Consistent, Diverse Sets (CoSP)",
        "Batched In-Context Examples (Dense)",
        "Step Verification (Diverse)",
        "Maximizing Mutual Information",
        "Meta-Chain of Thought (Meta-CoT)",
        "Specialized Experts",
        "Self-Consistency",
        "Universal Self-Consistency",
        "Task-Specific Selection (USP)",
        "Prompt Paraphrasing",
        "Chain of Verification (CoV)",
        "Self-Calibration",
        "Self-Refinement (Self-Refine)",
        "Self-Verification",
        "Reverse Chain of Thought (Reverse CoT)",
        "Cumulative Reasoning",
        "Functional Decomposition",
        "Faithful Chain of Thought (Faithful CoT)",
        "Least-to-Most",
        "Plan and Solve",
        "Program of Thought",
        "Recursive Thought (Recurs.-of-Thought)",
        "Skeleton of Thought",
        "Tree of Thought (ToT)"
    ]
    
    missing_techniques = []
    for technique in expected_techniques:
        if technique not in llms_techniques:
            missing_techniques.append(technique)
    
    if missing_techniques:
        print(f"‚ùå Missing techniques: {missing_techniques}")
        return False
    else:
        print(f"‚úÖ All {len(expected_techniques)} expected techniques found!")
        
    # Test technique structure
    for name, technique in llms_techniques.items():
        if not isinstance(technique, dict):
            print(f"‚ùå Technique {name} is not a dictionary")
            return False
        if 'definition' not in technique:
            print(f"‚ùå Technique {name} missing definition")
            return False
        if 'example' not in technique:
            print(f"‚ùå Technique {name} missing example")
            return False
    
    print("‚úÖ All techniques have proper structure (definition + example)")
    return True

async def test_technique_selection():
    """Test that technique selection works for different question types"""
    print("\nüß™ Testing technique selection...")
    
    test_cases = [
        {
            "question": "How do I debug this React component that's not rendering?",
            "expected_types": ["debugging"],
            "expected_techniques_contain": ["Step-Back Prompting"]  # More realistic expectation
        },
        {
            "question": "What's the best architecture for a microservices system?", 
            "expected_types": ["architecture"],
            "expected_techniques_contain": ["Plan and Solve"]  # More realistic expectation
        },
        {
            "question": "How can I optimize this slow database query?",
            "expected_types": ["optimization"],
            "expected_techniques_contain": ["Plan and Solve"]  # More realistic expectation
        },
        {
            "question": "Compare React vs Vue vs Angular for a large enterprise application",
            "expected_types": ["comparison"],
            "expected_techniques_contain": ["Tabular Chain of Thought (Tabular CoT)"]  # More realistic expectation
        },
        {
            "question": "Explain how machine learning algorithms work and implement a simple neural network",
            "expected_types": ["explanation", "implementation"],
            "expected_techniques_contain": ["Least-to-Most"]  # More realistic expectation
        }
    ]
    
    for i, test_case in enumerate(test_cases):
        print(f"\n  Test {i+1}: {test_case['question'][:50]}...")
        
        try:
            selection = await select_optimal_techniques(test_case["question"])
            
            print(f"    Selected techniques: {selection.selected_techniques}")
            print(f"    Question types: {selection.question_types}")
            print(f"    Complexity level: {selection.complexity_level}")
            print(f"    Reasoning: {selection.reasoning}")
            
            # Verify expected question types
            for expected_type in test_case["expected_types"]:
                if expected_type not in selection.question_types:
                    print(f"    ‚ùå Expected question type '{expected_type}' not found")
                    return False
            
            # Verify expected techniques
            for expected_technique in test_case["expected_techniques_contain"]:
                if expected_technique not in selection.selected_techniques:
                    print(f"    ‚ùå Expected technique '{expected_technique}' not selected")
                    return False
            
            # Verify we have reasonable number of techniques (1-5)
            if len(selection.selected_techniques) < 1 or len(selection.selected_techniques) > 5:
                print(f"    ‚ùå Unreasonable number of techniques: {len(selection.selected_techniques)}")
                return False
                
            print(f"    ‚úÖ Test {i+1} passed")
            
        except Exception as e:
            print(f"    ‚ùå Test {i+1} failed with error: {e}")
            return False
    
    print("\n‚úÖ All technique selection tests passed!")
    return True

async def test_fallback_techniques():
    """Test that fallback techniques are comprehensive"""
    print("\nüß™ Testing fallback techniques...")
    
    fallback_techniques = get_comprehensive_fallback_techniques()
    
    print(f"‚úÖ Fallback contains {len(fallback_techniques)} techniques")
    
    # Verify fallback has all major technique categories
    expected_categories = [
        "Chain of Thought", "Few-Shot", "Role Assignment", "Self-Consistency",
        "Tree of Thought", "Plan and Solve", "Step-Back Prompting"
    ]
    
    found_categories = []
    for technique_name in fallback_techniques.keys():
        for category in expected_categories:
            if category.lower() in technique_name.lower():
                found_categories.append(category)
                break
    
    if len(found_categories) < 5:  # Should have at least 5 major categories
        print(f"‚ùå Fallback missing major technique categories. Found: {found_categories}")
        return False
    
    print(f"‚úÖ Fallback contains major technique categories: {found_categories}")
    return True

async def test_full_pipeline():
    """Test the full pipeline with a sample question"""
    print("\nüß™ Testing full pipeline...")
    
    try:
        # Initialize components
        await initialize_components()
        
        # Test a complex question
        test_question = "I'm building a real-time chat application with React and Node.js. The messages are loading slowly and users are complaining. How should I optimize the performance and what architecture patterns should I use?"
        
        print(f"Test question: {test_question}")
        
        # Test technique selection
        selection = await select_optimal_techniques(test_question)
        
        print(f"Selected techniques: {selection.selected_techniques}")
        print(f"Question types detected: {selection.question_types}")
        print(f"Complexity level: {selection.complexity_level}")
        print(f"Reasoning: {selection.reasoning}")
        
        # Verify we got reasonable results
        if len(selection.selected_techniques) == 0:
            print("‚ùå No techniques selected")
            return False
            
        if "optimization" not in selection.question_types and "architecture" not in selection.question_types:
            print("‚ùå Failed to detect optimization/architecture question types")
            return False
            
        print("‚úÖ Full pipeline test passed!")
        return True
        
    except Exception as e:
        print(f"‚ùå Full pipeline test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

async def generate_technique_report():
    """Generate a detailed report of all loaded techniques"""
    print("\nüìä Generating technique report...")
    
    await load_llms_txt_techniques()
    
    report = {
        "total_techniques": len(llms_techniques),
        "techniques": {}
    }
    
    for name, technique in llms_techniques.items():
        report["techniques"][name] = {
            "definition_length": len(technique.get("definition", "")),
            "example_length": len(technique.get("example", "")),
            "has_definition": "definition" in technique,
            "has_example": "example" in technique
        }
    
    # Save report
    with open("technique_report.json", "w") as f:
        json.dump(report, f, indent=2)
    
    print(f"‚úÖ Report saved to technique_report.json")
    print(f"   Total techniques: {report['total_techniques']}")
    
    # Summary statistics
    techniques_with_both = sum(1 for t in report["techniques"].values() if t["has_definition"] and t["has_example"])
    avg_def_length = sum(t["definition_length"] for t in report["techniques"].values()) / len(report["techniques"])
    avg_ex_length = sum(t["example_length"] for t in report["techniques"].values()) / len(report["techniques"])
    
    print(f"   Techniques with both definition & example: {techniques_with_both}")
    print(f"   Average definition length: {avg_def_length:.1f} characters")
    print(f"   Average example length: {avg_ex_length:.1f} characters")

async def main():
    """Run all tests"""
    print("üöÄ Starting Comprehensive Technique Testing\n")
    
    tests = [
        ("Technique Loading", test_technique_loading),
        ("Technique Selection", test_technique_selection), 
        ("Fallback Techniques", test_fallback_techniques),
        ("Full Pipeline", test_full_pipeline)
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        print(f"\n{'='*50}")
        print(f"Running: {test_name}")
        print('='*50)
        
        try:
            result = await test_func()
            if result:
                passed += 1
                print(f"‚úÖ {test_name} PASSED")
            else:
                print(f"‚ùå {test_name} FAILED")
        except Exception as e:
            print(f"‚ùå {test_name} FAILED with exception: {e}")
            import traceback
            traceback.print_exc()
    
    # Generate report
    await generate_technique_report()
    
    print(f"\n{'='*50}")
    print(f"FINAL RESULTS: {passed}/{total} tests passed")
    print('='*50)
    
    if passed == total:
        print("üéâ ALL TESTS PASSED! The server is ready for deployment.")
        return True
    else:
        print("‚ùå Some tests failed. Please review the issues above.")
        return False

if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1) 